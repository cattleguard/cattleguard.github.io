<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R on Tony On Risk</title>
    <link>/tags/r/index.xml</link>
    <description>Recent content in R on Tony On Risk</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>My Name</copyright>
    <atom:link href="/tags/r/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>How To Apply Google&#39;s CausalImpact Package to Analyze Infosec Intervention</title>
      <link>/2018/07/12/counter-factual-infosec/</link>
      <pubDate>Thu, 12 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/07/12/counter-factual-infosec/</guid>
      <description>&lt;p&gt;Google released their CausalImpact package a few years ago and when they did my mind started racing with ideas for information security and information risk applications.&lt;/p&gt;
&lt;p&gt;Imagine if you could propose a control, policy change or process improvement with an expected effect on a response variable, which would lead you to purposefully defining a way to measure intervention outcomes. Not bad. Now you go on to determine a number of covariates. You’re a risk management mad scientist with a knack for catchy blog titles.&lt;/p&gt;
&lt;p&gt;Here’s one example. You implement a policy to reduce a type of ticket coming into the service desk. You have a pre-period before the intervention (policy in place) and post-intervention. You have the count of the target ticket type over time and a likely covariate might be the volume of all other ticket types over that same period.&lt;/p&gt;
&lt;p&gt;I know all this talk of counterfactual computation, blah blah, might feel a bit uncomfortable, but Google tried their best to make this dead simple to start playing with (just look at &lt;code&gt;summary(impact, &amp;quot;report&amp;quot;)&lt;/code&gt;). Generating alternate realities is now one of my most endorsed LinkedIN skills.&lt;/p&gt;
&lt;p&gt;Before going any further I recommend checking out &lt;a href=&#34;https://youtu.be/GTgZfCltMm8&#34;&gt;this video.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;So, let’s go beat a dead horse and take a look at the pre and post-period for the Target breach using a couple of unaffected retailers as predictors. &lt;em&gt;This isn’t a great example, there are likely a bucket of confounders that make this less than convincing, but it will at least illustrate usage.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;First make sure you have the CausalImpact installed. &lt;code&gt;install.packages(&amp;quot;CausalImpact&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(CausalImpact)
library(quantmod)
library(ggplot2)
library(zoo)

# Symbols for Target, Walmart, and Costco
symbols.of.interest &amp;lt;- c(&amp;quot;TGT&amp;quot;, &amp;quot;WMT&amp;quot;, &amp;quot;COST&amp;quot;)

prices &amp;lt;- new.env()

# Ticker prices are for weekdays only, so in order to use dates in pre/post we have to fix the index.
allDates &amp;lt;- seq.Date(
       min(as.Date(&amp;quot;2013-01-01&amp;quot;)),
       max(as.Date(&amp;quot;2014-12-31&amp;quot;)),
       &amp;quot;day&amp;quot;)

getSymbols(symbols.of.interest, from = as.Date(&amp;quot;2013-01-01&amp;quot;), to = as.Date(&amp;quot;2014-12-31&amp;quot;), env = prices, auto.assign = T)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;[1] “TGT” “WMT” “COST”&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
# December 18, 2013 the news of Target&amp;#39;s mega-breach was made public

pre.period &amp;lt;- as.Date(c(&amp;quot;2013-01-02&amp;quot;, &amp;quot;2013-12-19&amp;quot;))
post.period &amp;lt;- as.Date(c(&amp;quot;2013-12-20&amp;quot;, &amp;quot;2014-12-30&amp;quot;))

retail.stocks &amp;lt;- merge(prices$TGT$TGT.Close, prices$WMT$WMT.Close, prices$COST$COST.Close)

# make a zoo out of the complete dates
complete.days &amp;lt;- zoo(allDates, order.by = allDates)
big.join &amp;lt;- merge.zoo(complete.days, retail.stocks, all = TRUE, fill = NA)
big.join &amp;lt;- big.join[,colnames(big.join) != &amp;quot;complete.days&amp;quot;]
# we can&amp;#39;t have NA in our response variable column, so we take the Google advice of approximating the value
big.join &amp;lt;- na.approx(big.join)

impact &amp;lt;- CausalImpact(big.join, pre.period, post.period)
plot(impact)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/counter-factual-infosec_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I know quite well that my naïveté is likely on full display for serious data scientists and statisticians. Please feel free to drop me a note with clarifications and corrections. I very much enjoy learning on these topics.&lt;/p&gt;
&lt;p&gt;Thanks to Google and as always everyone keeping it going on StackOverflow.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data Cleaning Practice Pt1 - Steak Stats</title>
      <link>/2017/11/17/data-cleaning-practice-pt1/</link>
      <pubDate>Fri, 17 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/17/data-cleaning-practice-pt1/</guid>
      <description>&lt;p&gt;Our family is in the habit of heading west.  As such, we&amp;rsquo;ve passed through Amarillo, TX a time or two.  I&amp;rsquo;ve even stopped into the &lt;a href=&#34;https://www.bigtexan.com/&#34;&gt;Big Texan&lt;/a&gt; once for breakfast.&lt;/p&gt;

&lt;p&gt;For miles down I-40 you&amp;rsquo;ll see billboards for this restaurant and brewery advertising a free 72oz steak, if you can eat it.&lt;/p&gt;

&lt;p&gt;An office debate erupted amongst two of my colleagues over just what types of people end up beating the beast.&lt;/p&gt;

&lt;p&gt;They argued.  I fired up rvest.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://gist.github.com/cattleguard/9a6536222af4bd9a8964cdf0602141a4&#34;&gt;Here&amp;rsquo;s a gist leveraging rvest to create a dataset from the Hall of Fame.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Now we have a great starting point for answering some questions, but you may notice a few issues.  It&amp;rsquo;s almost like this contest is for fun or something.  It&amp;rsquo;s as if settling an office debate, R style was not a primary goal.&lt;/p&gt;

&lt;p&gt;Pretty much every column needs to be cleaned up.  My current workflow is to use grep() with invert = TRUE to show everything that doesn&amp;rsquo;t match my expected pattern.  Here&amp;rsquo;s an example: &lt;code&gt;grep(&amp;quot;^[0-9]{1,3}$&amp;quot;, big.texan.recordbook$age, value = TRUE, invert = TRUE)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;So, here&amp;rsquo;s my attempt at &lt;a href=&#34;https://gist.githubusercontent.com/cattleguard/2274938cd22f05c364f7eaddfec83f7c/raw/c0f049d5e6e29e8238b867d465c497d5e69e0a28/big_texan_clean.R&#34;&gt;cleanup&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This appeared to be close enough for our purposes.  I always enjoy this type of freestyle practice.  I did pick up a TIL function along the way: &lt;code&gt;gsubfn()&lt;/code&gt; which allows you to take your regex capture group and then apply a function to it.  I used this to address the kg conversion in the weight column.  I feel like there&amp;rsquo;s probably a &lt;code&gt;%&amp;gt;%&lt;/code&gt; equivalent and I&amp;rsquo;d love to see an example if you&amp;rsquo;ve got the time.&lt;/p&gt;

&lt;p&gt;Now, I just post beefy stats from the dataset.
&lt;img src=&#34;/post/bigtexan/avg_age.png&#34; alt=&#34;average age&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>